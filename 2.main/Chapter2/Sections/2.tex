\section{تئوری اطلاعات کلاسیک}


در این قسمت به تعریف برخی مفاهیم مهم در تئوری اطلاعات کلاسیک می پردازیم. با توجه به اینکه نیاز ما در این پروژه به متغیر های تصادفی گسسته بوده، لذا این مفاهیم را صرفا برای متغیر های تصادفی گسسته تعریف کرده ایم. این مباحث تئوری از 
\cite{cover}
ذکر شده است.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{اطلاعات}
شانون مقدار کمّی برای اطلاعات موجود در پیشامد یا پیام
$m$
ارائه داد:
\input{\formulaPATH{2}{1}}
که در آن 
$p(m)$
احتمال رخ دادن پیشامد 
$m$
است. اگر پایه ی لگاریتم 2 باشد، اطلاعات بیان شده با واحد بیت خواهد بود.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{آنتروپی}
آنتروپی یک متغیر تصادفی گسسته ی 
$X$ 
به صورت زیر تعریف می شود.
\input{\formulaPATH{2}{2}}
که به آن می توان به صورت میانگین آماری اطلاعات متغیر تصادفی 
$X$
هم نگاه کرد.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{آنتروپی مشترک و شرطی}
آنتروپی مشترک
\LTRfootnote{Joint Entropy}
$H(X,Y)$
یک زوج متغیر تصادفی گسسته ی
$(X,Y)$
با توزیع مشترک
$p(x,y)$
به صورت زیر تعریف می شود:
\input{\formulaPATH{2}{3}}
اگر 
$(X,Y)$
 دارای توزیع مشترک
 $p(X,Y)$
 باشد، آن گاه آنتروپی شرطی
 \LTRfootnote{Conditional Entropy}
 $H(Y|X)$
 به صورت زیر تعریف می شود:
 \input{\formulaPATH{2}{4}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	
\subsection{آنتروپی نسبی و اطلاعات متقابل}
آنتروپی نسبی
 \LTRfootnote{Relative Entropy}
یا فاصله ی 
\lr{Kullback–Leibler}
میان دو تابع جرم احتمال به صورت زیر تعریف می شود:
\input{\formulaPATH{2}{5}}
در تعریف بالا، فرض کرده ایم که
$0 \log \frac{0}{0} =0$
و همچنین
$0 \log \frac{0}{q} =0$
و
$p \log \frac{p}{0} = \infty$.

دو متغیر تصادفی
$X$
و
$Y$
با تابع جرم احتمال مشترک
$p(x,y)$
و تابع های جرم احتمال حاشیه ای
$p(x)$
و
$p(y)$
را در نظر بگیرید.
اطلاعات متقابل
\LTRfootnote{Mutual Information}
$I(X;Y)$
آنتروپی نسبی بین توزیع مشترک و ضرب توزیع ها
$p(x) p(y)$
خواهد بود
\cite{cover}
:
\input{\formulaPATH{2}{6}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{کانال}
یک کانال گسسته به صورت یک سیستم متشکل از ورودی
$X$
و خروجی 
$Y$
به همراه ماتریس انتقال
$p(y|x)$
تعریف می شود که در آن
$p(y|x)$
احتمال مشاهده سمبل خروچی 
$y$
با فرض این که سمبل 
$x$
ارسال شود، می باشد.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{ظرفیت}
ظرفیت یک کانال کلاسیکی به صورت زیر تعریف می شود:
\input{\formulaPATH{2}{7}}
که در آن ماکسیمم گیری روی تمام توزیع احتمال های 
$p(x)$
ورودی گرفته شده است.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{قضیه ی شانون در کدینگ کانال}
برای یک کانال گسسته بدون حافظه با ظرفیت
$C$
، تمامی نرخ های کمتر از
$C$
قابل دستیابی می باشد. به خصوص برای هر نرخ
$R < C$
، دنباله ای از کد های 
$(2^{nR},n)$
وجود دارد که بیشینه خطای آن به صفر میل میکند	
$\lambda ^ {(n)} \rightarrow 0$.

همچنین، هر دنباله ای از کد های 
$(2^{nR},n)$
با
$\lambda ^ {(n)} \rightarrow 0$
باید حتما نرخی کوچکتر و یا مساوی ظرفیت کانال داشته باشد
$R \leq C$.
\cite{cover}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%