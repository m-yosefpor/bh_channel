\section{اطلاعات، آنتروپی و ظرفیت}
%what inf is
%what entropy is
%what capacity is

در این قسمت، به بررسی مفاهیم مورد نیاز در تئوری اطلاعات می پردازیم. به جرات میتوان مفهوم اطلاعات را یکی از مهمترین مفاهیم تئوری اطلاعات دانست. شانون توانست برای اطلاعات مقدار کمّی ارایه بدهد که با اینکار تاثیر شگرفی بر دنیای مخابرات بگذارد. مقدار اطلاعات موجود در یک پیشامد با عکس احتمال رخ دادن آن پیشامد متناسب است. 

\begin{equation}
	I(m) \propto \frac{1}{p(m)}
\end{equation}
به عبارتی هرچه از رخ دادن پیشامدی بیشتر شگفت زده شویم، اطلاعات بیشتری از آن پیشامد به دست می آوریم. این احتمال فقط یک پیشامد است. لذا میانگین اطلاعاتی که ما از یک متغیر تصادفی به دست می آوریم، میانگین آماری این اطلاعات است. 
\begin{equation}
	H(X) = E_p I(m)
\end{equation}

با توجه به این که این عبارت با آنتروپی تعریف شده در ترمودینامیک مشابه است، شانون به پیشنهاد فون نویمان
\LTRfootnote{John von Neumann}
نام آنتروپی را برای این میانگین در نظر گرفت
\cite{intro}.
در فصل بعد، به صورت دقیق به تعریف کمی اطلاعات و سایر مفاهیم مرتبط در تئوری اطلاعات می پردازیم.

از طرفی، یک کانال مخابراتی، محیطی است که سیگنال یا پیام وارد شده به آن با تغییراتی از آن خارج می شود. پس از نظر ریاضی، یک کانال در واقع یک نگاشت است که سیگنال ورودی را به سیگنال خروجی می نگارد. ظرفیت یک کانال به بیان نادقیق، بیشینه نرخ اطلاعاتی است که می توان اطلاعات را بدون از دست رفتن منتقل کرد. لذا اگر ظرفیت یک کانال مثبت باشد، بدین معنی است که می توان هر مقدار اطلاعات را توسط آن انتقال داد (اگرچه زمان این انتقال ممکن است بسیار طولانی باشد) و اگر ظرفیت کانال صفر باشد، بدین معنی است که نمی توان اطلاعاتی را توسط این کانال مخابره کرد یا به عبارت دیگر، اطلاعات وارد شده به کانال، در کانال از بین می رود و نمی توان این اطلاعات را از خروجی کانال بازیابی کرد.